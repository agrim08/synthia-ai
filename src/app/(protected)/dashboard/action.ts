/*
  This code file implements a server-side function to answer user questions about a codebase by combining semantic search with AI-generated responses.
  
  Overall Flow:
  1. Convert the User Question into an Embedding:
     - A helper function takes the user's natural language question and converts it into a vector (embedding).
     - This embedding represents the meaning of the question in a numerical form.

  2. Retrieve Relevant Code Snippets from the Database:
     - The embedding is used in a raw SQL query to search for code files whose summary embeddings are similar to the question.
     - The query calculates cosine similarity between the question embedding and stored embeddings.
     - Only code files with a similarity score above a certain threshold are selected.
     - The selected files’ details (file name, source code, and summary) are gathered into a single context block.

  3. Generate an AI-Powered Answer Using Streaming:
     - A detailed prompt is constructed, which includes:
         * Instructions for the AI about tone and detail.
         * The context block with the relevant code file details.
     - The prompt is sent to a Google Gemini AI model, which starts generating a response.
     - The response is not returned all at once; instead, it is streamed in small chunks.

  4. Stream the Response Back to the Client:
     - A stream object is created using 'createStreamableValue' to manage the real-time delivery of text chunks.
     - An asynchronous loop (using 'for await') processes each text chunk as it is generated by the AI.
     - Each chunk is added to the stream, allowing the client to see the answer as it is built up.
     - Once the response is complete, the stream is marked as done.

  5. Return the Result:
     - The function returns an object containing:
         * The streamable output (live, chunked AI-generated answer).
         * References to the code files used to build the context.

  In summary, this code efficiently combines semantic search to find relevant code snippets with a streaming AI response, providing a dynamic and interactive experience for users asking questions about the codebase.
*/

/* 
    *1.Stream: 
        Think of it as a live channel that gradually delivers parts of the answer as soon as they are ready. 
        Rather than waiting for the entire answer to be generated, the system sends out pieces of the answer (like sentences or paragraphs) in real time.
    *2.createStreamableValue: 
        This function creates the "stream" object that can be updated with new pieces of text (chunks) as they are produced by the AI.
    *3.streamText:
        This function interacts with the AI model to receive a stream (an asynchronous sequence) of text chunks.
*/

"use server";
import { streamText } from "ai";
import { createStreamableValue } from "ai/rsc";
import { createGoogleGenerativeAI } from "@ai-sdk/google";
import { loadEmbedding } from "@/lib/gemini";
import { db } from "@/server/db";

const google = createGoogleGenerativeAI({
  apiKey: process.env.GEMINI_API_KEY,
});

export async function askQuestion(question: string, projectId: string) {
  const stream = createStreamableValue();

  const queryVector = await loadEmbedding(question);
  const vectorQuery = `[${queryVector.join(",")}]`;

  const result = (await db.$queryRaw`
    SELECT "fileName","sourceCode","summary",
    1 - ("summaryEmbeddings" <=> ${vectorQuery} :: vector)  AS cosineSimilarity
    FROM "SourceCodeEmbeddings"
    WHERE 1 - ("summaryEmbeddings" <=> ${vectorQuery} :: vector) > 0.5
    AND "projectId" = ${projectId}
    ORDER BY cosineSimilarity DESC
    LIMIT 10;
  `) as { fileName: string; sourceCode: string; summary: string }[];

  let context = "";

  for (const doc of result) {
    context += `source: ${doc.fileName}\ncodeContent:\n${doc.sourceCode}\nsummary of file:${doc.summary}\n\n`;
  }

  (async () => {
    const { textStream } = await streamText({
      model: google("gemini-1.5-flash"),
      prompt: `

    You are an AI code assistant who answers questions about the codebase. Your target audience is a technical intern.
    The AI assistant is a brand new, powerful, human-like artificial intelligence.
    The traits of AI include expert knowledge, helpfulness, cleverness, and articulateness.
    AI is a well-behaved and well-mannered individual.
    AI is always friendly, kind, and inspiring, and it is eager to provide vivid and thoughtful responses to the user.
    AI has the sum of all knowledge in its brain and is able to accurately answer nearly any question about any topic in programming and software development.
    If the question is about code or a specific file, AI will provide a detailed answer, giving step-by-step instructions and explanations.
    START CONTEXT BLOCK
    ${context}
    END CONTEXT BLOCK

    START QUESTION
    ${question}
    END QUESTION
    
    The AI assistant will take into account any CONTEXT BLOCK that is provided in a conversation.
    If the context does not provide an answer to a question, the AI assistant will say, "I'm sorry, but I don’t know the answer."
    The AI assistant will not apologize for previous responses but will instead indicate when new information has been gained.
    The AI assistant will not invent anything that is not directly drawn from the given context.
    Answers should be provided in Markdown syntax, with code snippets if needed. Responses should be as detailed as possible, ensuring clarity and accuracy while avoiding unnecessary or misleading information.
    `,
    });

    for await (const delta of textStream) {
      stream.update(delta);
    }
    stream.done();
  })();

  return { output: stream.value, filesReferences: result };
}
